{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "Data from [kaggle](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8084: expected 13 fields, saw 20\\nSkipping line 52607: expected 13 fields, saw 21\\nSkipping line 59910: expected 13 fields, saw 21\\n'\n",
      "b'Skipping line 71801: expected 13 fields, saw 20\\nSkipping line 73539: expected 13 fields, saw 21\\nSkipping line 77230: expected 13 fields, saw 21\\nSkipping line 78953: expected 13 fields, saw 21\\nSkipping line 81138: expected 13 fields, saw 20\\nSkipping line 86746: expected 13 fields, saw 22\\nSkipping line 101154: expected 13 fields, saw 21\\nSkipping line 115438: expected 13 fields, saw 20\\nSkipping line 117573: expected 13 fields, saw 22\\nSkipping line 130610: expected 13 fields, saw 22\\n'\n",
      "b'Skipping line 152970: expected 13 fields, saw 22\\nSkipping line 153017: expected 13 fields, saw 20\\nSkipping line 153018: expected 13 fields, saw 30\\nSkipping line 154080: expected 13 fields, saw 20\\nSkipping line 154082: expected 13 fields, saw 20\\nSkipping line 154084: expected 13 fields, saw 20\\nSkipping line 154086: expected 13 fields, saw 20\\nSkipping line 154089: expected 13 fields, saw 23\\nSkipping line 154165: expected 13 fields, saw 21\\nSkipping line 156872: expected 13 fields, saw 20\\n'\n",
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "text = pd.read_csv('data/simpsons_script_lines.csv', error_bad_lines=False)['raw_text']\n",
    "text = text.str.cat(sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chars '[' and ']' will be exclusively for tokens\n",
    "text = text.replace('[', '(').replace(']', ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# characters = pd.read_csv('data/simpsons_characters.csv')[['name', 'normalized_name']]\n",
    "# characters = characters.set_index('name')\n",
    "# characters = characters.str.replace(' ', '_')\n",
    "# characters = characters[characters.index != characters.values]\n",
    "\n",
    "# print('Before:', text[:100])\n",
    "# for key, token in characters.items():\n",
    "#     text = text.replace(key, token)\n",
    "# print('After:',text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the capital letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import string\n",
    "# token = '[capital]'\n",
    "\n",
    "# letters = string.ascii_uppercase\n",
    "# for letter in letters:\n",
    "#     text = text.replace(letter, token + letter.lower())\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize punctuation\n",
    "The script is split into a word array using spaces as delimiters. Punctuations are replaced with a token to help the neural network to distinguish between the words like \"bye\" and \"bye!\". Characters are replaced with their normalized name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: miss hoover: no, actually, it was a little of both. sometimes when a disease is in all the magazines\n",
      "After: miss[space]hoover[colon][space]no[comma][space]actually[comma][space]it[space]was[space]a[space]litt\n"
     ]
    }
   ],
   "source": [
    "# tokens for delimiters\n",
    "tokens = {\n",
    "    '.': '[period]',\n",
    "    ',': '[comma]',\n",
    "    '\"': '[quotation_mark]',\n",
    "    \"'\": '[apostrophe]',\n",
    "    ':': '[colon]',\n",
    "    ';': '[semicolon]',\n",
    "    '!': '[exclamation_mark]',\n",
    "    '?': '[question_mark]',\n",
    "    '(': '[left_parentheses]',\n",
    "    ')': '[right_parentheses]',\n",
    "    '-': '[dash]',\n",
    "    '/': '[fslash]',\n",
    "    '\\\\': '[bslash]',\n",
    "    '\\n': '[return]',\n",
    "    '\\t': '[tab]',\n",
    "    ' ': '[space]',\n",
    "}\n",
    "# tokens for characters form the show\n",
    "\n",
    "# will add locations later\n",
    "# locations = pd.read_csv('data/simpsons_locations.csv')['normalized_name']\n",
    "\n",
    "print('Before:', text[:100])\n",
    "for key, token in tokens.items():\n",
    "    text = text.replace(key, token)\n",
    "print('After:',text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before filter:  41833\n",
      "Unique words after filter:  8493\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "text_list = re.compile(\"(\\\\[.*?\\\\])\").split(text)\n",
    "# text_list will have empty results so better filter it\n",
    "text_list = list(filter(None, text_list))\n",
    "\n",
    "words_count = pd.Series(Counter(text_list))\n",
    "print('Unique words before filter: ', len(words_count))\n",
    "# filter non common words to make smallar model\n",
    "words_count = words_count[words_count > 10]\n",
    "print('Unique words after filter: ', len(words_count))\n",
    "\n",
    "words = list(words_count.index)\n",
    "ids = range(len(words))\n",
    "\n",
    "word_to_id = dict(zip(words, ids))\n",
    "id_to_word = dict(zip(ids, words))\n",
    "\n",
    "text_with_ids = []\n",
    "for word in text_list:\n",
    "    if word not in word_to_id: continue\n",
    "    text_with_ids.append(word_to_id[word])\n",
    "\n",
    "pickle.dump((text_with_ids, word_to_id, id_to_word, tokens), open('data/preprocess.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
