{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T01:55:33.501091Z",
     "start_time": "2017-09-21T01:55:33.212787Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T01:55:33.730690Z",
     "start_time": "2017-09-21T01:55:33.614122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mpreprocess.p\u001b[0m*             \u001b[01;32msimpsons_episodes.csv\u001b[0m*   \u001b[01;32msimpsons_script_lines.csv\u001b[0m*\r\n",
      "\u001b[01;32msimpsons_characters.csv\u001b[0m*  \u001b[01;32msimpsons_locations.csv\u001b[0m*  vectors.h5\r\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "Data from [kaggle](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T01:55:36.065148Z",
     "start_time": "2017-09-21T01:55:35.374418Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8084: expected 13 fields, saw 20\\nSkipping line 52607: expected 13 fields, saw 21\\nSkipping line 59910: expected 13 fields, saw 21\\n'\n",
      "b'Skipping line 71801: expected 13 fields, saw 20\\nSkipping line 73539: expected 13 fields, saw 21\\nSkipping line 77230: expected 13 fields, saw 21\\nSkipping line 78953: expected 13 fields, saw 21\\nSkipping line 81138: expected 13 fields, saw 20\\nSkipping line 86746: expected 13 fields, saw 22\\nSkipping line 101154: expected 13 fields, saw 21\\nSkipping line 115438: expected 13 fields, saw 20\\nSkipping line 117573: expected 13 fields, saw 22\\nSkipping line 130610: expected 13 fields, saw 22\\n'\n",
      "b'Skipping line 152970: expected 13 fields, saw 22\\nSkipping line 153017: expected 13 fields, saw 20\\nSkipping line 153018: expected 13 fields, saw 30\\nSkipping line 154080: expected 13 fields, saw 20\\nSkipping line 154082: expected 13 fields, saw 20\\nSkipping line 154084: expected 13 fields, saw 20\\nSkipping line 154086: expected 13 fields, saw 20\\nSkipping line 154089: expected 13 fields, saw 23\\nSkipping line 154165: expected 13 fields, saw 21\\nSkipping line 156872: expected 13 fields, saw 20\\n'\n",
      "/home/evo/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "text = pd.read_csv('data/simpsons_script_lines.csv', error_bad_lines=False)['raw_text']\n",
    "text = text.str.cat(sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T01:56:04.296786Z",
     "start_time": "2017-09-21T01:56:04.268197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'ve invited us to their homes.\\n\\nLisa Simpson: But Mom, I want to hear the witty banter of sophisticated adults.\\n\\nBart Simpson: Yeah, you can't have any fun in bed.\\n\\nHomer Simpson: (KNOWING CHUCKLE) Oh son, when you're older, you'll know better.\\n\\nHomer Simpson: Hmmm. (SMACKS HIS LIPS) Oh, baby! Mmmm. Yeah.\\n\\nMarge Simpson: (FLUSTERED) Oh! They're here! How does everything look?\\n\\nHomer Simpson: How do I look?\\n\\nMarge Simpson: Do we have enough glasses?\\n\\nHomer Simpson: Do we have enough gag ice cubs?\\n\\nMarge Simpson: Homer! Homer! Put a record on!\\n\\nHomer Simpson: What are all our friends names again?\\n\\nMarge Simpson: Children! Go!\\n\\nNed Flanders: Hey, anybody mind if I serve as bartender? You know, I have a Ph.D in Mixology. (LAUGHS)\\n\\nMoe Szyslak: (UNDER BREATH) College boy.\\n\\nNed Flanders: Hey, Homer! Care to try some of my Flanders Planters punch?\\n\\nHomer Simpson: Why not? I paid for it.\\n\\nHomer \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[8000:][100:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T01:56:10.475807Z",
     "start_time": "2017-09-21T01:56:07.464120Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Miss Hoover: No, actually, it was a little of both. Sometimes when a disease is in all the magazines\n",
      "After: ['[capital]', 'miss', '[space]', '[capital]', 'hoover', '[colon]', '[space]', '[capital]', 'no', '[comma]', '[space]', 'actually', '[comma]', '[space]', 'it', '[space]', 'was', '[space]', 'a', '[space]', 'little', '[space]', 'of', '[space]', 'both', '[period]', '[space]', '[capital]', 'sometimes', '[space]', 'when', '[space]', 'a', '[space]', 'disease', '[space]', 'is', '[space]', 'in', '[space]', 'all', '[space]', 'the', '[space]', 'magazines', '[space]', 'and', '[space]', 'all', '[space]', 'the', '[space]', 'news', '[space]', 'shows', '[comma]', '[space]', 'it', '[apostrophe]', 's', '[space]', 'only', '[space]', 'natural', '[space]', 'that', '[space]', 'you', '[space]', 'think', '[space]', 'you', '[space]', 'have', '[space]', 'it', '[period]', '[return]', '[return]', '[capital]', 'lisa', '[space]', '[capital]', 'simpson', '[colon]', '[space]', '[left_parentheses]', '[capital]', 'near', '[space]', '[capital]', 'tears', '[right_parentheses]', '[space]', '[capital]', 'where', '[apostrophe]', 's', '[space]', '[capital]']\n"
     ]
    }
   ],
   "source": [
    "print('Before:', text[:100])\n",
    "text = text_to_tokens(text)\n",
    "print('After:', text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T01:56:24.398262Z",
     "start_time": "2017-09-21T01:56:10.477753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before filter:  41818\n",
      "Unique words after filter:  12664\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "words_count = pd.Series(Counter(text))\n",
    "print('Unique words before filter: ', len(words_count))\n",
    "\n",
    "# filter non common words to make smallar model\n",
    "words_count = words_count[words_count > 5]\n",
    "print('Unique words after filter: ', len(words_count))\n",
    "\n",
    "words = list(words_count.index)\n",
    "words.append('not_in_vocab')\n",
    "\n",
    "ids = range(len(words))\n",
    "\n",
    "word_to_id = dict(zip(words, ids))\n",
    "id_to_word = dict(zip(ids, words))\n",
    "\n",
    "en = spacy.load('en_core_web_md')\n",
    "\n",
    "no_vect_counter = 0\n",
    "vectors = {}\n",
    "for word in words:\n",
    "    word_id = word_to_id[word]\n",
    "    if word in token_to_value:\n",
    "        word = token_to_value[word]\n",
    "    if en.vocab[word].has_vector:\n",
    "        vectors[word_id] = en.vocab[word].vector\n",
    "    else:\n",
    "        no_vect_counter += 1\n",
    "        vec = np.random.uniform(-2, 2, 300)\n",
    "        vectors[word_id] = vec\n",
    "\n",
    "vectors = pd.DataFrame(vectors).T.sort_index()\n",
    "\n",
    "text_with_ids = []\n",
    "for word in text:\n",
    "    if word not in word_to_id:\n",
    "        word = 'not_in_vocab'\n",
    "    text_with_ids.append(word_to_id[word])\n",
    "\n",
    "assert len(vectors) == len(words)\n",
    "\n",
    "vectors.to_hdf(key='data', path_or_buf='data/vectors.h5')\n",
    "pickle.dump((text_with_ids, word_to_id, id_to_word), open('data/preprocess.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "85px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
