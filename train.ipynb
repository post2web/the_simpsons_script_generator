{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Hyper parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T03:10:34.630727Z",
     "start_time": "2017-09-21T03:10:34.604379Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "use_gpu_number = 0\n",
    "num_epochs = 100\n",
    "num_steps = 30000\n",
    "batch_size = 64\n",
    "rnn_size = 512 * 2\n",
    "embed_dim = 300\n",
    "n_layers = 1\n",
    "seq_length = 50\n",
    "learning_rate = .001\n",
    "lstm_keep_prob = .9\n",
    "embedd_keep_prob = 1.\n",
    "embedd_trainable = True\n",
    "checkout_dir = 'checkpoints' + str(use_gpu_number) + '/'\n",
    "\n",
    "os.makedirs(checkout_dir, exist_ok=True)\n",
    "%env CUDA_VISIBLE_DEVICES=$use_gpu_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T03:10:36.572277Z",
     "start_time": "2017-09-21T03:10:34.633485Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Use TensorFlow 1.0 or newer'\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found! To train this neural network could take days on CPU.')\n",
    "else:\n",
    "    print('Default GPU Device:', tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T03:10:36.815162Z",
     "start_time": "2017-09-21T03:10:36.574109Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5595231 12665\n"
     ]
    }
   ],
   "source": [
    "text, words_to_ids, ids_to_words = pickle.load(open('data/preprocess.p', mode='rb'))\n",
    "\n",
    "word_vectors = pd.read_hdf(key='data', path_or_buf='data/vectors.h5')\n",
    "\n",
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    x = np.array(int_text[:-1], dtype=np.int32)\n",
    "    y = np.array(int_text[1:], dtype=np.int32)\n",
    "\n",
    "    dim1 = len(x) // (batch_size * seq_length)\n",
    "\n",
    "    trim_len = len(x) - batch_size * dim1 * seq_length\n",
    "\n",
    "    x = x[:-trim_len]\n",
    "    y = y[:-trim_len]\n",
    "\n",
    "    x = np.split(x.reshape(batch_size, -1), dim1, 1)\n",
    "    y = np.split(y.reshape(batch_size, -1), dim1, 1)\n",
    "\n",
    "    result = np.array(list(zip(x, y)))\n",
    "    return result\n",
    "\n",
    "print(len(text), len(ids_to_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Sequence to sequence network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T03:10:42.122749Z",
     "start_time": "2017-09-21T03:10:36.817191Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "vocab_size = len(ids_to_words)\n",
    "\n",
    "# inputs\n",
    "inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "inputs_shape = tf.shape(inputs)\n",
    "\n",
    "targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "lr = tf.placeholder(tf.float32, [], name='learning')\n",
    "\n",
    "# embeddings\n",
    "#params = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1., 1.))\n",
    "\n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embed_dim]), trainable=embedd_trainable, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embed_dim])\n",
    "embedding_init = W.assign(embedding_placeholder)\n",
    "\n",
    "output_keep_prob = tf.placeholder_with_default(1., shape=[])\n",
    "embeddings_keep_prob = tf.placeholder_with_default(1., shape=[])\n",
    "\n",
    "#embeddings = tf.nn.embedding_lookup(params, inputs)\n",
    "embeddings = tf.nn.embedding_lookup(W, inputs)\n",
    "\n",
    "tf.contrib.layers.dropout(embeddings, keep_prob=embeddings_keep_prob)\n",
    "\n",
    "# recurent nn\n",
    "layers = []\n",
    "for _ in range(n_layers):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=output_keep_prob)\n",
    "    layers.append(cell)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "initial_state = cell.zero_state(inputs_shape[0], tf.float32)\n",
    "initial_state = tf.identity(initial_state, 'initial_state') # just to name it\n",
    "\n",
    "# output and state\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, embeddings, dtype=tf.float32)\n",
    "final_state = tf.identity(final_state, 'final_state')\n",
    "\n",
    "logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "\n",
    "# Probabilities for generating words\n",
    "probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "# Loss function\n",
    "cost = seq2seq.sequence_loss(\n",
    "    logits,\n",
    "    targets,\n",
    "    tf.ones([inputs_shape[0], inputs_shape[1]]))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# Gradient Clipping\n",
    "gradients = optimizer.compute_gradients(cost)\n",
    "capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Create the session & init\n",
    "\n",
    "I initialize the word embedings with pretrained embedings from word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T03:10:42.661649Z",
     "start_time": "2017-09-21T03:10:42.124873Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver(max_to_keep=1000)\n",
    "\n",
    "sess.run(embedding_init, feed_dict={embedding_placeholder: word_vectors});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Function used to generate some output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T03:10:42.717913Z",
     "start_time": "2017-09-21T03:10:42.664547Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from tokenizer import *\n",
    "\n",
    "def pick_word(probabilities, int_to_vocab):\n",
    "    probabilities[words_to_ids['not_in_vocab']] = 0.\n",
    "    # make it sum to 1\n",
    "    probabilities /= probabilities.sum()\n",
    "    word_id = np.random.choice(np.arange(len(probabilities)), size=1, p=probabilities)[0]\n",
    "    #word_id = np.argmax(probabilities)\n",
    "    return int_to_vocab[word_id]\n",
    "\n",
    "def generate(starting_text='Homer Simpson:', generate_length=300):\n",
    "    sentence_tokens = text_to_tokens(starting_text)\n",
    "    prev_state = sess.run(initial_state, {inputs: np.array([[1]])})\n",
    "\n",
    "    for n in range(generate_length):\n",
    "        sentence_ids = [[words_to_ids[word] for word in sentence_tokens]]\n",
    "        sentence_len = len(sentence_ids[0])\n",
    "\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {inputs: sentence_ids, initial_state: prev_state})\n",
    "\n",
    "        predicted_token = pick_word(probabilities[0][sentence_len-1], ids_to_words)\n",
    "        sentence_tokens.append(predicted_token)\n",
    "    return tokens_to_text(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 8,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Train the network\n",
    "\n",
    "The network will ouput some generated text on every 1000 steps. At first it would make much sense but after some time of training will get better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T03:55:26.532815Z",
     "start_time": "2017-09-21T03:10:42.720118Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 31,
        "hidden": false,
        "row": 8,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500 Train Loss: 2.58822 Valid Loss: 2.17844 in 44.54 secs\n",
      "Step: 1000 Train Loss: 2.33351 Valid Loss: 2.03975 in 44.01 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog don't have more a way. He's sent you a lot lettuce of that book time in.\n",
      "\n",
      "Homer Simpson: (\n",
      "**************************************************\n",
      "Bart Simpson: (Reading, Guard, \", Crossing Crazed For To Kent The Money, Weak Donuts Cause I I'\n",
      "**************************************************\n",
      "Moe Szyslak: Just all why the Live.\n",
      "\n",
      "Homer Simpson: Right of Mommy!\n",
      "\n",
      "Lionel Leonard: Done off.\n",
      "\n",
      "Lisa \n",
      "##################################################\n",
      "Step: 1500 Train Loss: 2.22151 Valid Loss: 1.9756 in 44.95 secs\n",
      "Step: 2000 Train Loss: 1.937 Valid Loss: 1.93837 in 44.22 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog think by what are you gonna go laughing?\n",
      "\n",
      "Homer Simpson: Bring yourself in.\n",
      "\n",
      "Carl Carlson: Lenny State Love\n",
      "**************************************************\n",
      "Bart Simpson: (Interrupting Something'S ) I finally think you got to lad and we lived out of Tv am her, but that\n",
      "**************************************************\n",
      "Moe Szyslak: (Small Eyed, His Cheek) Yeah, now I told your friend Blue Cowboys. I couldn't have backwards violating\n",
      "##################################################\n",
      "Step: 2500 Train Loss: 1.90753 Valid Loss: 1.91718 in 43.98 secs\n",
      "Step: 3000 Train Loss: 1.89093 Valid Loss: 1.89527 in 44.79 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog?\n",
      "\n",
      "Gabbo: Now I've gotta speak around to Bart's gum... as a cop.\n",
      "\n",
      "Arnie Ziff: (\n",
      "**************************************************\n",
      "Bart Simpson: (Embarrassed) Heh. Does you want you to untie a dirty doorbell and the Laffy Williams?\n",
      "\n",
      "Homer Simpson: (\n",
      "**************************************************\n",
      "Moe Szyslak: Wrong, we'll start my favorite anniversary.\n",
      "\n",
      "Miss Crowley: (Waving Out) Ow.\n",
      "\n",
      "Ghostly!\n",
      "\n",
      "Moe Szyslak:\n",
      "##################################################\n",
      "Step: 3500 Train Loss: 1.8353 Valid Loss: 1.88863 in 44.18 secs\n",
      "Step: 4000 Train Loss: 1.816 Valid Loss: 1.87618 in 44.34 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog crazy? (Points) I'm finally having a jerk.\n",
      "\n",
      "Grampa Simpson: Well, it looks like taffy of snakes. Not\n",
      "**************************************************\n",
      "Bart Simpson: Your name, Homer!\n",
      "\n",
      "Roger Meyers, Jr.: The time I can't say: George!... De-yours\n",
      "**************************************************\n",
      "Moe Szyslak: Sorry, Krusty.\n",
      "\n",
      "Bart Simpson: I Want'T... I love you, mister.\n",
      "\n",
      "(Rover Bar \n",
      "##################################################\n",
      "Step: 4500 Train Loss: 1.80432 Valid Loss: 1.86825 in 46.90 secs\n",
      "Step: 5000 Train Loss: 1.76878 Valid Loss: 1.86058 in 45.46 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog shirt?\n",
      "\n",
      "Homer Simpson: (Baby Talk) It wouldn't matter!\n",
      "\n",
      "(Simpson Home: int. simpson house - morning)\n",
      "**************************************************\n",
      "Bart Simpson: (Haw) Hey, remember a big sax truck.\n",
      "\n",
      "Seymour Skinner: (Confused) Wait, the fun I've had a\n",
      "**************************************************\n",
      "Moe Szyslak: Oh, no goodness! It's smart bird exercise!\n",
      "\n",
      "Grampa Simpson: Marge, I have such horrible friends.\n",
      "\n",
      "(Bart\n",
      "##################################################\n",
      "Step: 5500 Train Loss: 1.75315 Valid Loss: 1.85859 in 45.66 secs\n",
      "Step: 6000 Train Loss: 1.74324 Valid Loss: 1.86026 in 47.98 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog do at me? (Slams Fist) \"Get a six-trip now.\"\n",
      "\n",
      "Announcer: Man is come on nothing. She'\n",
      "**************************************************\n",
      "Bart Simpson: Wow, there's you know how to be bad.\n",
      "\n",
      "Bart Simpson: I had been right now, but they won't \n",
      "**************************************************\n",
      "Moe Szyslak: (Proudly, Then) We've never kicked an fit like Disney Francisco. Copyright!\n",
      "\n",
      "Cora: 'Cause Bart, you\n",
      "##################################################\n",
      "Step: 6500 Train Loss: 1.71324 Valid Loss: 1.85807 in 46.21 secs\n",
      "Step: 7000 Train Loss: 1.7015 Valid Loss: 1.85342 in 44.33 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog Flanders, friends and produce it?\n",
      "\n",
      "Lisa Simpson: Church? Where are you planning a flag with beer?\n",
      "\n",
      "Marge Simpson\n",
      "**************************************************\n",
      "Bart Simpson: Well, we didn't have just nails and shake that hands right back and eat the shock, lady (Chuckle Then Sighs)\n",
      "**************************************************\n",
      "Moe Szyslak: Thank you, honey.\n",
      "\n",
      "Rev. Timothy Lovejoy: Manjula, thy turtle down the damn genius board show is the same experience \n",
      "##################################################\n",
      "Step: 7500 Train Loss: 1.69044 Valid Loss: 1.85229 in 44.79 secs\n",
      "Step: 8000 Train Loss: 1.65879 Valid Loss: 1.85767 in 44.63 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog have?\n",
      "\n",
      "Marge Simpson: (Annoyed Murmur) There were Lisa, and the miracle nature is slowly -- except the state \n",
      "**************************************************\n",
      "Bart Simpson: (Dazed Badly) Sorry, I didn't want from one single worker, dye the waste. Who do that any? Mm\n",
      "**************************************************\n",
      "Moe Szyslak: (Distraught) Okay, the Rapture! Nice Dad.\n",
      "\n",
      "Homer Simpson: Marge, we haven't decided to play with that\n",
      "##################################################\n",
      "Step: 8500 Train Loss: 1.65227 Valid Loss: 1.85361 in 44.81 secs\n",
      "Step: 9000 Train Loss: 1.64182 Valid Loss: 1.85579 in 44.78 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog won't?\n",
      "\n",
      "Lisa Simpson: (Angry Groan)\n",
      "\n",
      "Fat Tony: Now I know my mother never liked.\n",
      "\n",
      "Homer\n",
      "**************************************************\n",
      "Bart Simpson: Good morning, everybody! (In To Sky) Ow! Owww look! You're the one.\n",
      "\n",
      "Body Announcer: (\n",
      "**************************************************\n",
      "Moe Szyslak: I won't tell Dad my mind is in there.\n",
      "\n",
      "Parson: You mean that?\n",
      "\n",
      "Homer Simpson: Uh, it\n",
      "##################################################\n",
      "Step: 9500 Train Loss: 1.60417 Valid Loss: 1.86061 in 44.84 secs\n",
      "Step: 10000 Train Loss: 1.60419 Valid Loss: 1.86418 in 45.19 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog get?\n",
      "\n",
      "C. Montgomery Burns: (Awestruck) No. You don't see the face: excellent. You've been longer\n",
      "**************************************************\n",
      "Bart Simpson: (Breathing Avalanche)\n",
      "\n",
      "Otto Mann: Guess you're not interested. I know it kisses you!\n",
      "\n",
      "Bart Simpson: \n",
      "**************************************************\n",
      "Moe Szyslak: You can enjoy now, honey.\n",
      "\n",
      "Lisa Simpson: It was some worse than the activities of Capital City Hall, the\n",
      "##################################################\n",
      "Step: 10500 Train Loss: 1.59272 Valid Loss: 1.87088 in 44.48 secs\n",
      "Step: 11000 Train Loss: 1.58496 Valid Loss: 1.86449 in 44.48 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog have to leave me?\n",
      "\n",
      "Homer Simpson: (Chorus) Say what I mean, sounds at it, it's Roy, \n",
      "**************************************************\n",
      "Bart Simpson: Oh, that's what.\n",
      "\n",
      "Homer Simpson: (Passing) God find me like is her.\n",
      "\n",
      "Waylon Smithers: Attitude,\n",
      "**************************************************\n",
      "Moe Szyslak: And now you'll mourn them with Hitler, but that's the drill and a cocktail tongue. Then I'm the \n",
      "##################################################\n",
      "Step: 11500 Train Loss: 1.55816 Valid Loss: 1.87687 in 43.65 secs\n",
      "Step: 12000 Train Loss: 1.54305 Valid Loss: 1.88784 in 44.68 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog going?\n",
      "\n",
      "Bart Simpson: (Taunting) Screw you, everybody!\n",
      "\n",
      "Bart Simpson: (Gleeful Laugh) My face!\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Bart Simpson: But I kinda appreciate being alive. The winner \"Krusty Pie Love\" and He was a bit weakness because/-We\n",
      "**************************************************\n",
      "Moe Szyslak: (Screams)\n",
      "\n",
      "(Maxine'S Skybox: ext. Tom U. Simpson and another show set - a little later)\n",
      "\n",
      "\n",
      "##################################################\n",
      "Step: 12500 Train Loss: 1.53597 Valid Loss: 1.89216 in 44.58 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 13000 Train Loss: 1.51109 Valid Loss: 1.89545 in 43.55 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog beat?\n",
      "\n",
      "(Night Gallery-Type Set: Int. heaven's clothing store - day)\n",
      "\n",
      "Selma Bouvier: (Nostalgic) \n",
      "**************************************************\n",
      "Bart Simpson: Listen, Mom, this is the Springfield Habitat. The top channel's almost too long.\n",
      "\n",
      "Bart Simpson: D'oh\n",
      "**************************************************\n",
      "Moe Szyslak: And I've never realized this to that girl before Carl like the music store.\n",
      "\n",
      "Homer Simpson: Quimby.\n",
      "\n",
      "Homer\n",
      "##################################################\n",
      "Step: 13500 Train Loss: 1.49673 Valid Loss: 1.91323 in 44.01 secs\n",
      "Step: 14000 Train Loss: 1.48899 Valid Loss: 1.91255 in 44.19 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog please leave, what else did you got?\n",
      "\n",
      "Homer Simpson: (Meekly) This knocks just thinking this is you do.\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Bart Simpson: There's polite to building it with the last Indian commercial.\n",
      "\n",
      "Lisa Simpson: Greetings, soccer is French class, and \n",
      "**************************************************\n",
      "Moe Szyslak: Now if they're at this station, here's the prize for freedom...\n",
      "\n",
      "Martin Prince: He went through my executive\n",
      "##################################################\n",
      "Step: 14500 Train Loss: 1.46615 Valid Loss: 1.92791 in 45.34 secs\n",
      "Step: 15000 Train Loss: 1.45175 Valid Loss: 1.93291 in 44.22 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog hit again? Everything's a great idea.\n",
      "\n",
      "Lisa Simpson: I agree.\n",
      "\n",
      "Bart Simpson: (Disgusted Noise, Then\n",
      "**************************************************\n",
      "Bart Simpson: (Stunned) Is her kind of dragging me up?\n",
      "\n",
      "Marge Simpson: (Annoyed Murmur)\n",
      "\n",
      "(Gas Station: Ext.\n",
      "**************************************************\n",
      "Moe Szyslak: Yeah, I promised I too.\n",
      "\n",
      "Marge Simpson: Well Bart, you can stay in the retirement home to out for\n",
      "##################################################\n",
      "Step: 15500 Train Loss: 1.4442 Valid Loss: 1.94619 in 46.33 secs\n",
      "Step: 16000 Train Loss: 1.42225 Valid Loss: 1.94901 in 46.14 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog get us?\n",
      "\n",
      " You Act In Sleep/Dog, Screams And Tongue-Glove Noises)\n",
      "\n",
      "Helen Dr. \n",
      "**************************************************\n",
      "Bart Simpson: Ah, up here, Randall Curtis. We haven't seen anything since she made me. No, but I believe in \n",
      "**************************************************\n",
      "Moe Szyslak: What the? How'd you got that, Homer? It won't go on and all she is after you.\n",
      "\n",
      "Bont\n",
      "##################################################\n",
      "Step: 16500 Train Loss: 1.40957 Valid Loss: 1.96626 in 44.65 secs\n",
      "Step: 17000 Train Loss: 1.40322 Valid Loss: 1.96416 in 46.99 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog make me get--\n",
      "\n",
      "Homer Simpson: (Short Scream)\n",
      "\n",
      "Grampa Simpson: Ohh, Homie! Do you know what \n",
      "**************************************************\n",
      "Bart Simpson: Sure, no. You can't remember me so much, if I could hand a store that fell in his place from a\n",
      "**************************************************\n",
      "Moe Szyslak: Don't worry. I don't want that you get us in the up of jail. Can't you're kissing Apu\n",
      "##################################################\n",
      "Step: 17500 Train Loss: 1.38253 Valid Loss: 1.97945 in 44.46 secs\n",
      "Step: 18000 Train Loss: 1.37328 Valid Loss: 1.97173 in 44.27 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog talk in her?\n",
      "\n",
      "Lisa Simpson: Her testicle?\n",
      "\n",
      "Lisa Simpson: (Dreamy) Honey...\n",
      "\n",
      "Homer Simpson: \n",
      "**************************************************\n",
      "Bart Simpson: Bart, help me.\n",
      "\n",
      "Bart Simpson: (Whispering) Former of Moe's Club; I don't get a phone \n",
      "**************************************************\n",
      "Moe Szyslak: He got on the curb... I need to know where the wife went into a little...\n",
      "\n",
      "Grampa Simpson: \n",
      "##################################################\n",
      "Step: 18500 Train Loss: 1.36667 Valid Loss: 1.98981 in 43.55 secs\n",
      "Step: 19000 Train Loss: 1.34605 Valid Loss: 2.01635 in 43.73 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog invented American eggs ...\n",
      "\n",
      "Marge Simpson: Homer, cheese!\n",
      "\n",
      "Homer Simpson: Nice work, Dad. C'mon,\n",
      "**************************************************\n",
      "Bart Simpson: Woo hoo! Milhouse! Milhouse!\n",
      "\n",
      "Milhouse Van Houten: (Deep Breath) Heh-heh. I saw you over the\n",
      "**************************************************\n",
      "Moe Szyslak: Eh, yes!\n",
      "\n",
      "C. Montgomery Burns: Simpson!\n",
      "\n",
      "Homer Simpson: I'll take it from man who made who\n",
      "##################################################\n",
      "Step: 19500 Train Loss: 1.34197 Valid Loss: 2.01722 in 47.99 secs\n",
      "Step: 20000 Train Loss: 1.33651 Valid Loss: 2.0379 in 47.93 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog '?\n",
      "\n",
      "Lisa Simpson: (Shakes Head) We're a little surprised for me. See you later.\n",
      "\n",
      "Bart Simpson\n",
      "**************************************************\n",
      "Bart Simpson: Man, you're a genius.\n",
      "\n",
      "Edna Krabappel-Flanders: I'm sure you'll find it I am quite a small\n",
      "**************************************************\n",
      "Moe Szyslak: Eh, no. One breaks at it!\n",
      "\n",
      "Singers: (Big Laugh) Yeah! / Yeah, Yeah! / It \n",
      "##################################################\n",
      "Step: 20500 Train Loss: 1.30631 Valid Loss: 2.03306 in 44.48 secs\n",
      "Step: 21000 Train Loss: 1.31545 Valid Loss: 2.03854 in 44.74 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog say?\n",
      "\n",
      "Marge Simpson: Simpson! How can you at Me Back? A boyfriend must really kill a boy you got\n",
      "**************************************************\n",
      "Bart Simpson: (Sings Louder \"Not\") Sputtering\n",
      "\n",
      "Bart Simpson: Dad, are you cold of fall? One, two, three. \n",
      "**************************************************\n",
      "Moe Szyslak: Let's roll.\n",
      "\n",
      "(Powers House: Ext. Sighs)\n",
      "\n",
      "Homer Simpson: He was so high.\n",
      "\n",
      "Gil Gunderson\n",
      "##################################################\n",
      "Step: 21500 Train Loss: 1.30901 Valid Loss: 2.03837 in 44.21 secs\n",
      "Step: 22000 Train Loss: 1.30474 Valid Loss: 2.04049 in 44.28 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog call him, Ned?\n",
      "\n",
      "Marge Simpson: (Casual) Oh, handsome.\n",
      "\n",
      "Homer Simpson: Now that I was gonna \n",
      "**************************************************\n",
      "Bart Simpson: This place is great. Do you have a bathroom?\n",
      "\n",
      "Milhouse Van Houten: (Chuckles)\n",
      "\n",
      "Old Jewish Man: \n",
      "**************************************************\n",
      "Moe Szyslak: Let me tell you, Homer.\n",
      "\n",
      "Moe Szyslak: (Calling After Him, Quickly) Hurry up, Simpson! There\n",
      "##################################################\n",
      "Step: 22500 Train Loss: 1.29234 Valid Loss: 2.08205 in 49.29 secs\n",
      "Step: 23000 Train Loss: 1.28323 Valid Loss: 2.06371 in 43.42 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog go into that she?\n",
      "\n",
      "Homer Simpson: I guess I'll drop her turn.\n",
      "\n",
      "Ned Flanders: Will you raise \n",
      "**************************************************\n",
      "Bart Simpson: You've broken, son.\n",
      "\n",
      "Bart Simpson: We'd like to sweep up this old country.\n",
      "\n",
      "Bart Simpson: Make\n",
      "**************************************************\n",
      "Moe Szyslak: (Singing) Who'S Who'S Business Is The Straw? / They Toke And Pray Back A\n",
      "##################################################\n",
      "Step: 23500 Train Loss: 1.28089 Valid Loss: 2.06053 in 44.35 secs\n",
      "Step: 24000 Train Loss: 1.27305 Valid Loss: 2.06977 in 45.50 secs\n",
      "##################################################\n",
      "Homer Simpson: Where did the dog hate?\n",
      "\n",
      "Homer Simpson: Woo hoo!\n",
      "\n",
      "Female Warrior: (Latin Hat) You could have one.\n",
      "\n",
      "Marge Simpson\n",
      "**************************************************\n",
      "Bart Simpson: Bye, Bart.\n",
      "\n",
      "Laura Powers: (Pats Wally) Oh don't worry, we'll just make it what else you\n",
      "**************************************************\n",
      "Moe Szyslak: (Friendly) Treehouse to over eighteen. (Chuckles)\n",
      "\n",
      "Moe Szyslak: Here's money!\n",
      "\n",
      "Homer Simpson: (Sarcastic) \n",
      "##################################################\n",
      "Step: 24500 Train Loss: 1.26121 Valid Loss: 2.08509 in 45.38 secs\n",
      "Step: 25000 Train Loss: 1.25834 Valid Loss: 2.08391 in 44.58 secs\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homer Simpson: Where did the dog in a bag of garbage?\n",
      "\n",
      "Seymour Skinner: Yes.\n",
      "\n",
      "Ralph Wiggum: I don't have private manager Bart Simpson\n",
      "**************************************************\n",
      "Bart Simpson: I don't want to be here for the English close Saturday. For once, you found a business report to the school\n",
      "**************************************************\n",
      "Moe Szyslak: Oh, you're hot, too. But at least I'm going to build this conversation.\n",
      "\n",
      "(Barn: Int. barn \n",
      "##################################################\n",
      "Step: 25500 Train Loss: 1.24863 Valid Loss: 2.08712 in 45.47 secs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2916ecb9ed51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0membeddings_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0membedd_keep_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         }\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_loss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batches = get_batches(text, batch_size, seq_length)\n",
    "\n",
    "train_batches = batches[:-len(batches) // 10]\n",
    "test_batches = batches[-len(batches) // 10:]\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    #np.random.shuffle(batches)\n",
    "    state = sess.run(initial_state, {inputs: train_batches[0][0]})\n",
    "    train_loss = []\n",
    "    for x, y in train_batches:\n",
    "        step += 1\n",
    "        start = time.time()\n",
    "        feed = {\n",
    "            inputs: x,\n",
    "            targets: y,\n",
    "            initial_state: state,\n",
    "            lr: learning_rate,\n",
    "            output_keep_prob:lstm_keep_prob,\n",
    "            embeddings_keep_prob: embedd_keep_prob\n",
    "        }\n",
    "        train_loss_, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "        train_loss.append(train_loss_)\n",
    "        took = time.time() - start\n",
    "        if step % 500 == 0:\n",
    "            train_took = took * 500\n",
    "            test_state = sess.run(initial_state, {inputs: test_batches[0][0]})\n",
    "            test_loss = []\n",
    "            for x, y in test_batches:\n",
    "                start = time.time()\n",
    "                feed = {\n",
    "                    inputs: x,\n",
    "                    targets: y,\n",
    "                    initial_state:\n",
    "                    test_state, lr: learning_rate\n",
    "                }\n",
    "                test_loss_, test_state = sess.run([cost, final_state], feed)\n",
    "                test_loss.append(test_loss_)\n",
    "                test_took = time.time() - start\n",
    "            \n",
    "            print('Step:', step,\n",
    "                  'Train Loss:', np.mean(train_loss),\n",
    "                  'Valid Loss:', np.mean(test_loss),\n",
    "                  \"in %.2f secs\" % train_took)\n",
    "\n",
    "            saver.save(sess, checkout_dir + 'model'+str(step))\n",
    "            \n",
    "        if step % 1000 == 0:\n",
    "            print('#' * 50)\n",
    "            print(generate(starting_text='Homer Simpson: Where did the dog', generate_length=50))\n",
    "            print('*' * 50)\n",
    "            print(generate(starting_text='Bart Simpson:', generate_length=50))\n",
    "            print('*' * 50)\n",
    "            print(generate(starting_text='Moe Szyslak:', generate_length=50))\n",
    "            print('#' * 50)\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "    if step >= num_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T03:55:32.476298Z",
     "start_time": "2017-09-21T03:55:32.472376Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
