{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# use only the first GPU\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Default GPU Device:', u'/gpu:0')\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Use TensorFlow 1.0 or newer'\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found! To train this neural network could take awhile.')\n",
    "else:\n",
    "    print('Default GPU Device:', tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5378116, 2824)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "text, words_to_ids, ids_to_words = pickle.load(open('data/preprocess.p', mode='rb'))\n",
    "print(len(text), len(ids_to_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    x = np.array(int_text[:-1], dtype=np.int32)\n",
    "    y = np.array(int_text[1:], dtype=np.int32)\n",
    "\n",
    "    dim1 = len(x) // (batch_size * seq_length)\n",
    "\n",
    "    trim_len = len(x) - batch_size * dim1 * seq_length\n",
    "\n",
    "    x = x[:-trim_len]\n",
    "    y = y[:-trim_len]\n",
    "\n",
    "    x = np.split(x.reshape(batch_size, -1), dim1, 1)\n",
    "    y = np.split(y.reshape(batch_size, -1), dim1, 1)\n",
    "\n",
    "    result = np.array(list(zip(x, y)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 500\n",
    "rnn_size = 500\n",
    "n_layers = 1\n",
    "seq_length = 60\n",
    "learning_rate = .001\n",
    "\n",
    "checkout_dir = 'checkpoints/'\n",
    "try:\n",
    "    os.makedirs(checkout_dir)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "vocab_size = len(ids_to_words)\n",
    "\n",
    "# inputs\n",
    "inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "inputs_shape = tf.shape(inputs)\n",
    "targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "lr = tf.placeholder(tf.float32, [], name='learning')\n",
    "\n",
    "# recurent nn\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([lstm]*n_layers)\n",
    "initial_state = cell.zero_state(inputs_shape[0], tf.float32)\n",
    "initial_state = tf.identity(initial_state, 'initial_state') # just to name it\n",
    "\n",
    "# embeddings\n",
    "params = tf.Variable(tf.random_uniform([vocab_size, rnn_size], -1., 1.))\n",
    "embeddings = tf.nn.embedding_lookup(params, inputs)\n",
    "\n",
    "# output and state\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, embeddings, dtype=tf.float32)\n",
    "final_state = tf.identity(final_state, 'final_state')\n",
    "\n",
    "logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "\n",
    "# Probabilities for generating words\n",
    "probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "# Loss function\n",
    "cost = seq2seq.sequence_loss(\n",
    "    logits,\n",
    "    targets,\n",
    "    tf.ones([inputs_shape[0], inputs_shape[1]]))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# Gradient Clipping\n",
    "gradients = optimizer.compute_gradients(cost)\n",
    "capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used to generate some output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tokenizer import *\n",
    "\n",
    "def pick_word(probabilities, int_to_vocab):\n",
    "    return int_to_vocab[np.argmax(probabilities)]\n",
    "\n",
    "def generate(starting_text='Homer Simpson:', generate_length=300):\n",
    "    sentence_tokens = text_to_tokens(starting_text)\n",
    "    prev_state = sess.run(initial_state, {inputs: np.array([[1]])})\n",
    "\n",
    "    for n in range(generate_length):\n",
    "        sentence_ids = [[words_to_ids[word] for word in sentence_tokens]]\n",
    "        sentence_len = len(sentence_ids[0])\n",
    "\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {inputs: sentence_ids, initial_state: prev_state})\n",
    "\n",
    "        predicted_token = pick_word(probabilities[0][sentence_len-1], ids_to_words)\n",
    "        sentence_tokens.append(predicted_token)\n",
    "    return tokens_to_text(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Step:', 500, 'Sequence Loss:', 1.8951381, 'took time:', '64.62 secs')\n",
      "('Step:', 1000, 'Sequence Loss:', 1.7698169, 'took time:', '65.05 secs')\n",
      "('Step:', 1500, 'Sequence Loss:', 1.7025232, 'took time:', '67.06 secs')\n",
      "('Step:', 2000, 'Sequence Loss:', 1.6722658, 'took time:', '64.07 secs')\n",
      "('Step:', 2500, 'Sequence Loss:', 1.6514809, 'took time:', '65.54 secs')\n",
      "('Step:', 3000, 'Sequence Loss:', 1.6166742, 'took time:', '64.91 secs')\n",
      "('Step:', 3500, 'Sequence Loss:', 1.5666766, 'took time:', '62.53 secs')\n",
      "('Step:', 4000, 'Sequence Loss:', 1.5578958, 'took time:', '62.64 secs')\n",
      "('Step:', 4500, 'Sequence Loss:', 1.5433419, 'took time:', '67.61 secs')\n",
      "('Step:', 5000, 'Sequence Loss:', 1.5414732, 'took time:', '67.31 secs')\n",
      "##################################################\n",
      "\n",
      "\n",
      "Homer Simpson: Where did the dog  you to the ?\n",
      "\n",
      "Homer Simpson: I don't know. I don't know what I did.\n",
      "\n",
      "Homer \n",
      "**************************************************\n",
      "Bart Simpson: I don't know what I did.\n",
      "\n",
      "Bart Simpson: I don't know what I did.\n",
      "\n",
      "Bart Simpson:\n",
      "**************************************************\n",
      "Moe: I don't know what I did.\n",
      "\n",
      "Homer Simpson: I don't know what I did.\n",
      "\n",
      "Homer Simpson:\n",
      "##################################################\n",
      "('Step:', 5500, 'Sequence Loss:', 1.4969509, 'took time:', '67.25 secs')\n",
      "('Step:', 6000, 'Sequence Loss:', 1.4920297, 'took time:', '67.24 secs')\n",
      "('Step:', 6500, 'Sequence Loss:', 1.4580463, 'took time:', '67.94 secs')\n",
      "('Step:', 7000, 'Sequence Loss:', 1.4613415, 'took time:', '67.29 secs')\n",
      "('Step:', 7500, 'Sequence Loss:', 1.4511142, 'took time:', '67.38 secs')\n",
      "('Step:', 8000, 'Sequence Loss:', 1.4319673, 'took time:', '67.28 secs')\n",
      "('Step:', 8500, 'Sequence Loss:', 1.4118739, 'took time:', '67.34 secs')\n",
      "('Step:', 9000, 'Sequence Loss:', 1.3876951, 'took time:', '67.46 secs')\n",
      "('Step:', 9500, 'Sequence Loss:', 1.3844525, 'took time:', '67.86 secs')\n",
      "('Step:', 10000, 'Sequence Loss:', 1.3458451, 'took time:', '67.29 secs')\n",
      "##################################################\n",
      "\n",
      "\n",
      "Homer Simpson: Where did the dog  the  ?\n",
      "\n",
      "Homer Simpson: I don't know. I just wanted to be a   and a  .\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Bart Simpson: I can't believe I got a  on the  .\n",
      "\n",
      "Bart Simpson: I can't believe I was  to \n",
      "**************************************************\n",
      "Moe: You know, I don't know how to  you with the  .\n",
      "\n",
      "Homer Simpson: I can't believe \n",
      "##################################################\n",
      "('Step:', 10500, 'Sequence Loss:', 1.3555428, 'took time:', '67.31 secs')\n",
      "('Step:', 11000, 'Sequence Loss:', 1.3348438, 'took time:', '67.34 secs')\n",
      "('Step:', 11500, 'Sequence Loss:', 1.3349346, 'took time:', '67.26 secs')\n",
      "('Step:', 12000, 'Sequence Loss:', 1.3076268, 'took time:', '67.36 secs')\n",
      "('Step:', 12500, 'Sequence Loss:', 1.3001188, 'took time:', '67.29 secs')\n",
      "('Step:', 13000, 'Sequence Loss:', 1.2971323, 'took time:', '67.59 secs')\n",
      "('Step:', 13500, 'Sequence Loss:', 1.2698479, 'took time:', '68.62 secs')\n",
      "('Step:', 14000, 'Sequence Loss:', 1.2460917, 'took time:', '67.44 secs')\n",
      "('Step:', 14500, 'Sequence Loss:', 1.1936682, 'took time:', '67.33 secs')\n",
      "('Step:', 15000, 'Sequence Loss:', 1.2667224, 'took time:', '66.72 secs')\n",
      "##################################################\n",
      "\n",
      "\n",
      "Homer Simpson: Where did the dog  the  with the ?\n",
      "\n",
      "Homer Simpson: I don't know. I don't know what to do.\n",
      "\n",
      "Homer \n",
      "**************************************************\n",
      "Bart Simpson: I don't know. I don't know what to do.\n",
      "\n",
      "Homer Simpson: I don't know what to do.\n",
      "**************************************************\n",
      "Moe: () I don't know what to do. I don't know what I did. I don't know what \n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batches = get_batches(text, batch_size, seq_length)\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    state = sess.run(initial_state, {inputs: batches[0][0]})\n",
    "    np.random.shuffle(batches)\n",
    "    for x, y in batches:\n",
    "        step += 1\n",
    "        feed = {inputs: x, targets: y, initial_state: state, lr: learning_rate}\n",
    "        start = time.time()\n",
    "        train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "        took = time.time() - start\n",
    "        if step % 500 == 0:\n",
    "            took *= 500\n",
    "            print('Step:', step, 'Sequence Loss:', train_loss, \"%.2f secs\" % took)\n",
    "            \n",
    "        if step % 5000 == 0:\n",
    "            saver.save(sess, checkout_dir + 'model')\n",
    "            print('#' * 50)\n",
    "            print(generate(starting_text='\\n\\nHomer Simpson: Where did the dog', generate_length=50))\n",
    "            print('*' * 50)\n",
    "            print(generate(starting_text='Bart Simpson:', generate_length=50))\n",
    "            print('*' * 50)\n",
    "            print(generate(starting_text='Moe:', generate_length=50))\n",
    "            print('#' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "('Step:', 500, 'Sequence Loss:', 1.9290897, 'took time:', '29.09 secs')\n",
    "('Step:', 1000, 'Sequence Loss:', 1.7947156, 'took time:', '28.46 secs')\n",
    "('Step:', 1500, 'Sequence Loss:', 1.7302173, 'took time:', '28.19 secs')\n",
    "('Step:', 2000, 'Sequence Loss:', 1.6997606, 'took time:', '31.83 secs')\n",
    "('Step:', 2500, 'Sequence Loss:', 1.708122, 'took time:', '28.33 secs')\n",
    "('Step:', 3000, 'Sequence Loss:', 1.6659172, 'took time:', '27.52 secs')\n",
    "('Step:', 3500, 'Sequence Loss:', 1.6495925, 'took time:', '27.50 secs')\n",
    "('Step:', 4000, 'Sequence Loss:', 1.5773289, 'took time:', '27.56 secs')\n",
    "('Step:', 4500, 'Sequence Loss:', 1.5948731, 'took time:', '27.45 secs')\n",
    "('Step:', 5000, 'Sequence Loss:', 1.6080065, 'took time:', '26.70 secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
