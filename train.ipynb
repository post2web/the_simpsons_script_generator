{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# use only the first GPU\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Use TensorFlow 1.0 or newer'\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found! To train this neural network could take awhile.')\n",
    "else:\n",
    "    print('Default GPU Device:', tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4704276 8493\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "text, words_to_ids, ids_to_words, token_dict = pickle.load(open('data/preprocess.p', mode='rb'))\n",
    "print(len(text), len(ids_to_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    x = np.array(int_text[:-1], dtype=np.int32)\n",
    "    y = np.array(int_text[1:], dtype=np.int32)\n",
    "\n",
    "    dim1 = len(x) // (batch_size * seq_length)\n",
    "\n",
    "    trim_len = len(x) - batch_size * dim1 * seq_length\n",
    "\n",
    "    x = x[:-trim_len]\n",
    "    y = y[:-trim_len]\n",
    "\n",
    "    x = np.split(x.reshape(batch_size, -1), dim1, 1)\n",
    "    y = np.split(y.reshape(batch_size, -1), dim1, 1)\n",
    "\n",
    "    result = np.array(list(zip(x, y)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 400\n",
    "batch_size = 100\n",
    "rnn_size = 2000\n",
    "seq_length = 500\n",
    "learning_rate = .001\n",
    "show_every_n_epochs = 1\n",
    "\n",
    "checkout_dir = 'checkpoints/'\n",
    "os.makedirs(checkout_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "vocab_size = len(ids_to_words)\n",
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # inputs\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    inputs_shape = tf.shape(inputs)\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, [], name='learning')\n",
    "\n",
    "    # recurent nn\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm])\n",
    "    initial_state = cell.zero_state(inputs_shape[0], tf.float32)\n",
    "    initial_state = tf.identity(initial_state, 'initial_state') # just to name it\n",
    "    \n",
    "    # embeddings\n",
    "    params = tf.Variable(tf.random_uniform([vocab_size, rnn_size], -1., 1.))\n",
    "    embeddings = tf.nn.embedding_lookup(params, inputs)\n",
    "    \n",
    "    # output and state\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embeddings, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, 'final_state')\n",
    "    \n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([inputs_shape[0], inputs_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 train_loss: 2.52678 time: 365.33\n",
      "Epoch: 1 train_loss: 2.25463 time: 370.72\n",
      "Epoch: 2 train_loss: 2.14637 time: 370.05\n",
      "Epoch: 3 train_loss: 2.07989 time: 368.42\n",
      "Epoch: 4 train_loss: 2.03941 time: 372.07\n",
      "Epoch: 5 train_loss: 1.99261 time: 370.46\n",
      "Epoch: 6 train_loss: 1.95724 time: 369.89\n",
      "Epoch: 7 train_loss: 1.92521 time: 372.46\n",
      "Epoch: 8 train_loss: 1.89262 time: 372.74\n",
      "Epoch: 9 train_loss: 1.8653 time: 373.93\n",
      "Epoch: 10 train_loss: 1.83624 time: 371.53\n",
      "Epoch: 11 train_loss: 1.80836 time: 369.72\n",
      "Epoch: 12 train_loss: 1.78733 time: 368.64\n",
      "Epoch: 13 train_loss: 1.7635 time: 373.40\n",
      "Epoch: 14 train_loss: 1.73545 time: 371.39\n",
      "Epoch: 15 train_loss: 1.7118 time: 373.41\n",
      "Epoch: 16 train_loss: 1.68674 time: 372.78\n",
      "Epoch: 17 train_loss: 1.66006 time: 376.00\n",
      "Epoch: 18 train_loss: 1.63692 time: 369.14\n",
      "Epoch: 19 train_loss: 1.61482 time: 371.29\n",
      "Epoch: 20 train_loss: 1.59468 time: 373.58\n",
      "Epoch: 21 train_loss: 1.56556 time: 372.23\n",
      "Epoch: 22 train_loss: 1.54162 time: 373.24\n",
      "Epoch: 23 train_loss: 1.51843 time: 370.40\n",
      "Epoch: 24 train_loss: 1.48994 time: 373.07\n",
      "Epoch: 25 train_loss: 1.46722 time: 372.03\n",
      "Epoch: 26 train_loss: 1.43837 time: 372.38\n",
      "Epoch: 27 train_loss: 1.4168 time: 373.28\n",
      "Epoch: 28 train_loss: 1.38802 time: 371.98\n",
      "Epoch: 29 train_loss: 1.35742 time: 373.21\n",
      "Epoch: 30 train_loss: 1.33204 time: 372.27\n",
      "Epoch: 31 train_loss: 1.31654 time: 371.86\n",
      "Epoch: 32 train_loss: 1.28097 time: 374.84\n",
      "Epoch: 33 train_loss: 1.24865 time: 373.64\n",
      "Epoch: 34 train_loss: 1.23485 time: 372.68\n",
      "Epoch: 35 train_loss: 1.20869 time: 370.19\n",
      "Epoch: 36 train_loss: 1.18902 time: 372.02\n",
      "Epoch: 37 train_loss: 1.14942 time: 370.67\n",
      "Epoch: 38 train_loss: 1.12609 time: 370.84\n",
      "Epoch: 39 train_loss: 1.09773 time: 369.94\n",
      "Epoch: 40 train_loss: 1.07729 time: 370.27\n",
      "Epoch: 41 train_loss: 1.04799 time: 371.67\n",
      "Epoch: 42 train_loss: 1.01779 time: 369.74\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batches = get_batches(text, batch_size, seq_length)\n",
    "sess = tf.Session(graph=train_graph)\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    state = sess.run(initial_state, {inputs: batches[0][0]})\n",
    "    start = time.time()\n",
    "\n",
    "    for _, (x, y) in enumerate(batches):\n",
    "        feed = {inputs: x, targets: y, initial_state: state, lr: learning_rate}\n",
    "        train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "    \n",
    "    if epoch % show_every_n_epochs == 0:\n",
    "        print('Epoch:', epoch, 'train_loss:', train_loss, 'time:', \"%.2f\" %  (time.time() - start))\n",
    "        with train_graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, checkout_dir + 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
